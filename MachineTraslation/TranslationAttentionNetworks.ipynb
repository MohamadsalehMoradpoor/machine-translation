{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881439d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9bde8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = keras.utils.get_file('spa-eng.zip', origin='https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f530726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hacker\\\\.keras\\\\datasets\\\\spa-eng.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e51b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = os.path.join(os.path.dirname(path_to_zip), 'spa-eng', 'spa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aadb26b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hacker\\\\.keras\\\\datasets\\\\spa-eng\\\\spa.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ecdd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1533b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.~,])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.~,]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "929001d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i have some friends to help . <end>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = \"I have some friends to help.\"\n",
    "preprocess_sentence(en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d39782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> aun no he dicho nada . <end>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_sentence = \"AÃºn no he dicho nada.\"\n",
    "preprocess_sentence(sp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7885ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f97d00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x17c882e4d40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataset(path_to_file, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f73fb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "en, sp = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8234386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118964"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ac84e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_lenght(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bae1bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14fd5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, target_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, target_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b753791",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer = load_dataset(path_to_file, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e211a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  97,   3, ...,   0,   0,   0],\n",
       "       [  1, 179,   3, ...,   0,   0,   0],\n",
       "       [  1, 466,   3, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  1,   7,  10, ...,   0,   0,   0],\n",
       "       [  1,   7,  10, ...,   0,   0,   0],\n",
       "       [  1,   7,  10, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b20f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = max_lenght(input_tensor), max_lenght(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16827cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d9bcaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(t, ' .... ', lang.index_word[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "875d75a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 97,  3,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b01b89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  ....  <start>\n",
      "97  ....  ve\n",
      "3  ....  .\n",
      "2  ....  <end>\n"
     ]
    }
   ],
   "source": [
    "convert(input_lang_tokenizer, input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71f9418c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<start>',\n",
       " 2: '<end>',\n",
       " 3: '.',\n",
       " 4: 'i',\n",
       " 5: 'you',\n",
       " 6: '?',\n",
       " 7: 'tom',\n",
       " 8: 'is',\n",
       " 9: 'it',\n",
       " 10: 's',\n",
       " 11: 'a',\n",
       " 12: 'he',\n",
       " 13: 't',\n",
       " 14: 'the',\n",
       " 15: 'we',\n",
       " 16: 'm',\n",
       " 17: 'me',\n",
       " 18: 're',\n",
       " 19: 'that',\n",
       " 20: 'this',\n",
       " 21: 'to',\n",
       " 22: 'do',\n",
       " 23: 'are',\n",
       " 24: 'can',\n",
       " 25: 'my',\n",
       " 26: 'they',\n",
       " 27: 'was',\n",
       " 28: 'she',\n",
       " 29: 'don',\n",
       " 30: 'have',\n",
       " 31: 'your',\n",
       " 32: 'go',\n",
       " 33: 'what',\n",
       " 34: 'in',\n",
       " 35: 'not',\n",
       " 36: 'll',\n",
       " 37: 'like',\n",
       " 38: 'here',\n",
       " 39: 'on',\n",
       " 40: 'him',\n",
       " 41: 'let',\n",
       " 42: 'be',\n",
       " 43: 'did',\n",
       " 44: 'know',\n",
       " 45: 'come',\n",
       " 46: 'up',\n",
       " 47: 'am',\n",
       " 48: 'want',\n",
       " 49: 'how',\n",
       " 50: ',',\n",
       " 51: 'mary',\n",
       " 52: 'get',\n",
       " 53: 'who',\n",
       " 54: 'very',\n",
       " 55: 'now',\n",
       " 56: 'need',\n",
       " 57: 'has',\n",
       " 58: 'please',\n",
       " 59: 'no',\n",
       " 60: 'there',\n",
       " 61: 'help',\n",
       " 62: 'her',\n",
       " 63: 'love',\n",
       " 64: 'at',\n",
       " 65: 'see',\n",
       " 66: 'just',\n",
       " 67: 'out',\n",
       " 68: 'his',\n",
       " 69: 've',\n",
       " 70: 'got',\n",
       " 71: 'for',\n",
       " 72: 'where',\n",
       " 73: 'look',\n",
       " 74: 'stop',\n",
       " 75: 'one',\n",
       " 76: 'us',\n",
       " 77: 'good',\n",
       " 78: 'car',\n",
       " 79: 'too',\n",
       " 80: 'so',\n",
       " 81: 'all',\n",
       " 82: 'why',\n",
       " 83: 'will',\n",
       " 84: 'an',\n",
       " 85: 'home',\n",
       " 86: 'of',\n",
       " 87: 'with',\n",
       " 88: 'give',\n",
       " 89: 'back',\n",
       " 90: 'were',\n",
       " 91: 'keep',\n",
       " 92: 'take',\n",
       " 93: 'dog',\n",
       " 94: 'saw',\n",
       " 95: 'didn',\n",
       " 96: 'isn',\n",
       " 97: 'may',\n",
       " 98: 'happy',\n",
       " 99: 'stay',\n",
       " 100: 'won',\n",
       " 101: 'work',\n",
       " 102: 'hate',\n",
       " 103: 'must',\n",
       " 104: 'wait',\n",
       " 105: 'leave',\n",
       " 106: 'again',\n",
       " 107: 'likes',\n",
       " 108: 'down',\n",
       " 109: 'feel',\n",
       " 110: 'book',\n",
       " 111: 'try',\n",
       " 112: 'made',\n",
       " 113: 'eat',\n",
       " 114: 'right',\n",
       " 115: 'them',\n",
       " 116: 'still',\n",
       " 117: 'had',\n",
       " 118: 'time',\n",
       " 119: 'going',\n",
       " 120: 'does',\n",
       " 121: 'money',\n",
       " 122: 'call',\n",
       " 123: 'say',\n",
       " 124: 'lost',\n",
       " 125: 'came',\n",
       " 126: 'tell',\n",
       " 127: 'went',\n",
       " 128: 'well',\n",
       " 129: 'today',\n",
       " 130: 'old',\n",
       " 131: 'busy',\n",
       " 132: 'looks',\n",
       " 133: 'ask',\n",
       " 134: 'away',\n",
       " 135: 'loves',\n",
       " 136: 'job',\n",
       " 137: 'man',\n",
       " 138: 'bad',\n",
       " 139: 'everyone',\n",
       " 140: 'never',\n",
       " 141: 'some',\n",
       " 142: 'over',\n",
       " 143: 'pay',\n",
       " 144: 'mine',\n",
       " 145: 'from',\n",
       " 146: 'ready',\n",
       " 147: 'alone',\n",
       " 148: 'read',\n",
       " 149: 'wrong',\n",
       " 150: 'room',\n",
       " 151: 'live',\n",
       " 152: 'angry',\n",
       " 153: 'tired',\n",
       " 154: 'talk',\n",
       " 155: 'more',\n",
       " 156: 'make',\n",
       " 157: 'could',\n",
       " 158: 'about',\n",
       " 159: 'nice',\n",
       " 160: 'nobody',\n",
       " 161: 'off',\n",
       " 162: 'french',\n",
       " 163: 'house',\n",
       " 164: 'hurt',\n",
       " 165: 'speak',\n",
       " 166: 'watch',\n",
       " 167: 'should',\n",
       " 168: 'left',\n",
       " 169: 'cold',\n",
       " 170: 'big',\n",
       " 171: 'late',\n",
       " 172: 'play',\n",
       " 173: 'new',\n",
       " 174: 'true',\n",
       " 175: 'and',\n",
       " 176: 'drink',\n",
       " 177: 'life',\n",
       " 178: 'our',\n",
       " 179: 'drunk',\n",
       " 180: 'lot',\n",
       " 181: 'open',\n",
       " 182: 'said',\n",
       " 183: 'these',\n",
       " 184: 'wasn',\n",
       " 185: 'boston',\n",
       " 186: 'way',\n",
       " 187: 'fast',\n",
       " 188: 'boy',\n",
       " 189: 'nothing',\n",
       " 190: 'turn',\n",
       " 191: 'understand',\n",
       " 192: 'ok',\n",
       " 193: 'hungry',\n",
       " 194: 'wants',\n",
       " 195: 'when',\n",
       " 196: 'really',\n",
       " 197: 'ate',\n",
       " 198: 'hurry',\n",
       " 199: 'died',\n",
       " 200: 'hot',\n",
       " 201: 'felt',\n",
       " 202: 'think',\n",
       " 203: 'name',\n",
       " 204: 'find',\n",
       " 205: 'sick',\n",
       " 206: 'yours',\n",
       " 207: 'everybody',\n",
       " 208: 'sit',\n",
       " 209: 'gave',\n",
       " 210: 'smart',\n",
       " 211: 'answer',\n",
       " 212: 'broke',\n",
       " 213: 'aren',\n",
       " 214: 'been',\n",
       " 215: 'hold',\n",
       " 216: 'miss',\n",
       " 217: 'stupid',\n",
       " 218: 'fun',\n",
       " 219: 'done',\n",
       " 220: 'sleep',\n",
       " 221: 'everything',\n",
       " 222: 'already',\n",
       " 223: 'listen',\n",
       " 224: 'fine',\n",
       " 225: 'bed',\n",
       " 226: 'first',\n",
       " 227: 'much',\n",
       " 228: 'sing',\n",
       " 229: 'loved',\n",
       " 230: 'coming',\n",
       " 231: 'great',\n",
       " 232: 'bought',\n",
       " 233: 'two',\n",
       " 234: 'father',\n",
       " 235: 'long',\n",
       " 236: 'cry',\n",
       " 237: 'married',\n",
       " 238: 'cat',\n",
       " 239: 'best',\n",
       " 240: 'tv',\n",
       " 241: 'hear',\n",
       " 242: 'friend',\n",
       " 243: 'ran',\n",
       " 244: 'stand',\n",
       " 245: 'crazy',\n",
       " 246: 'knows',\n",
       " 247: 'mad',\n",
       " 248: 'swim',\n",
       " 249: 'close',\n",
       " 250: 'idea',\n",
       " 251: 'easy',\n",
       " 252: 'rich',\n",
       " 253: 'hard',\n",
       " 254: 'walk',\n",
       " 255: 'start',\n",
       " 256: 'kill',\n",
       " 257: 'run',\n",
       " 258: 'put',\n",
       " 259: 'knew',\n",
       " 260: 'both',\n",
       " 261: 'doctor',\n",
       " 262: 'needs',\n",
       " 263: 'something',\n",
       " 264: 'almost',\n",
       " 265: 'wife',\n",
       " 266: 'only',\n",
       " 267: 'hair',\n",
       " 268: 'show',\n",
       " 269: 'by',\n",
       " 270: 'sad',\n",
       " 271: 'move',\n",
       " 272: 'remember',\n",
       " 273: 'dead',\n",
       " 274: 'gone',\n",
       " 275: 'crying',\n",
       " 276: 'drive',\n",
       " 277: 'free',\n",
       " 278: 'write',\n",
       " 279: 'school',\n",
       " 280: 'pretty',\n",
       " 281: 'those',\n",
       " 282: 'doesn',\n",
       " 283: 'trust',\n",
       " 284: 'tall',\n",
       " 285: 'd',\n",
       " 286: 'better',\n",
       " 287: 'friends',\n",
       " 288: 'fat',\n",
       " 289: 'sat',\n",
       " 290: 'bus',\n",
       " 291: 'bit',\n",
       " 292: 'heard',\n",
       " 293: 'water',\n",
       " 294: 'lie',\n",
       " 295: 'early',\n",
       " 296: 'called',\n",
       " 297: 'die',\n",
       " 298: 'hand',\n",
       " 299: 'met',\n",
       " 300: 'yourself',\n",
       " 301: 'bag',\n",
       " 302: 'teacher',\n",
       " 303: 'eyes',\n",
       " 304: 'bring',\n",
       " 305: 'lying',\n",
       " 306: 'yes',\n",
       " 307: 'day',\n",
       " 308: 'door',\n",
       " 309: 'believe',\n",
       " 310: 'doing',\n",
       " 311: 'enemy',\n",
       " 312: 'mean',\n",
       " 313: 'soon',\n",
       " 314: 'dogs',\n",
       " 315: 'wine',\n",
       " 316: 'buy',\n",
       " 317: 'as',\n",
       " 318: 'young',\n",
       " 319: 'monday',\n",
       " 320: 'singing',\n",
       " 321: 'red',\n",
       " 322: 'helped',\n",
       " 323: 'looked',\n",
       " 324: 'afraid',\n",
       " 325: 'someone',\n",
       " 326: 'quit',\n",
       " 327: 'guys',\n",
       " 328: 'use',\n",
       " 329: 'follow',\n",
       " 330: 'food',\n",
       " 331: 'found',\n",
       " 332: 'lucky',\n",
       " 333: 'key',\n",
       " 334: 'lunch',\n",
       " 335: 'tomorrow',\n",
       " 336: 'coffee',\n",
       " 337: 'kind',\n",
       " 338: 'hit',\n",
       " 339: 'safe',\n",
       " 340: 'forget',\n",
       " 341: 'study',\n",
       " 342: 'later',\n",
       " 343: 'whose',\n",
       " 344: 'seen',\n",
       " 345: 'enough',\n",
       " 346: 'running',\n",
       " 347: 'small',\n",
       " 348: 'son',\n",
       " 349: 'always',\n",
       " 350: 'enjoy',\n",
       " 351: 'might',\n",
       " 352: 'hope',\n",
       " 353: 'awake',\n",
       " 354: 'shot',\n",
       " 355: 'told',\n",
       " 356: 'fish',\n",
       " 357: 'arrived',\n",
       " 358: 'hat',\n",
       " 359: 'break',\n",
       " 360: 'short',\n",
       " 361: 'took',\n",
       " 362: 'quiet',\n",
       " 363: 'next',\n",
       " 364: 'works',\n",
       " 365: 'eating',\n",
       " 366: 'night',\n",
       " 367: 'asked',\n",
       " 368: 'cats',\n",
       " 369: 'milk',\n",
       " 370: 'seems',\n",
       " 371: 'dream',\n",
       " 372: 'dinner',\n",
       " 373: 'watching',\n",
       " 374: 'mother',\n",
       " 375: 'began',\n",
       " 376: 'thanks',\n",
       " 377: 'cool',\n",
       " 378: 'care',\n",
       " 379: 'real',\n",
       " 380: 'hurts',\n",
       " 381: 'cook',\n",
       " 382: 'check',\n",
       " 383: 'beer',\n",
       " 384: 'smoke',\n",
       " 385: 'hide',\n",
       " 386: 'fired',\n",
       " 387: 'rain',\n",
       " 388: 'send',\n",
       " 389: 'changed',\n",
       " 390: 'myself',\n",
       " 391: 'talking',\n",
       " 392: 'fault',\n",
       " 393: 'which',\n",
       " 394: 'hands',\n",
       " 395: 'yet',\n",
       " 396: 'brave',\n",
       " 397: 'full',\n",
       " 398: 'around',\n",
       " 399: 'liked',\n",
       " 400: 'problem',\n",
       " 401: 'win',\n",
       " 402: 'seat',\n",
       " 403: 'joking',\n",
       " 404: 'broken',\n",
       " 405: 'secret',\n",
       " 406: 'rest',\n",
       " 407: 'books',\n",
       " 408: 'hates',\n",
       " 409: 'english',\n",
       " 410: 'owe',\n",
       " 411: 'girl',\n",
       " 412: 'family',\n",
       " 413: 'would',\n",
       " 414: 'fell',\n",
       " 415: 'shy',\n",
       " 416: 'sorry',\n",
       " 417: 'along',\n",
       " 418: 'men',\n",
       " 419: 'kids',\n",
       " 420: 'naive',\n",
       " 421: 'reading',\n",
       " 422: 'working',\n",
       " 423: 'joke',\n",
       " 424: 'clean',\n",
       " 425: 'sure',\n",
       " 426: 'word',\n",
       " 427: 'little',\n",
       " 428: 'lied',\n",
       " 429: 'calm',\n",
       " 430: 'forgot',\n",
       " 431: 'warm',\n",
       " 432: 'patient',\n",
       " 433: 'missed',\n",
       " 434: 'begin',\n",
       " 435: 'snow',\n",
       " 436: 'needed',\n",
       " 437: 'woman',\n",
       " 438: 'lawyer',\n",
       " 439: 'beautiful',\n",
       " 440: 'happen',\n",
       " 441: 'kept',\n",
       " 442: 'lives',\n",
       " 443: 'child',\n",
       " 444: 'person',\n",
       " 445: 'tried',\n",
       " 446: 'shut',\n",
       " 447: 'fix',\n",
       " 448: 'failed',\n",
       " 449: 'hey',\n",
       " 450: 'promised',\n",
       " 451: 'anybody',\n",
       " 452: 'change',\n",
       " 453: 'funny',\n",
       " 454: 'once',\n",
       " 455: 'pen',\n",
       " 456: 'drank',\n",
       " 457: 'things',\n",
       " 458: 'plan',\n",
       " 459: 'own',\n",
       " 460: 'insane',\n",
       " 461: 'truth',\n",
       " 462: 'game',\n",
       " 463: 'born',\n",
       " 464: 'hang',\n",
       " 465: 'wake',\n",
       " 466: 'smiled',\n",
       " 467: 'cut',\n",
       " 468: 'boring',\n",
       " 469: 'empty',\n",
       " 470: 'kidding',\n",
       " 471: 'smell',\n",
       " 472: 'meet',\n",
       " 473: 'happened',\n",
       " 474: 'wish',\n",
       " 475: 'blue',\n",
       " 476: 'outside',\n",
       " 477: 'studying',\n",
       " 478: 'useless',\n",
       " 479: 'box',\n",
       " 480: 'somebody',\n",
       " 481: 'clever',\n",
       " 482: 'shoes',\n",
       " 483: 'fire',\n",
       " 484: 'moved',\n",
       " 485: 'perfect',\n",
       " 486: 'kiss',\n",
       " 487: 'laughed',\n",
       " 488: 'blind',\n",
       " 489: 'far',\n",
       " 490: 'worked',\n",
       " 491: 'dark',\n",
       " 492: 'thank',\n",
       " 493: 'anyone',\n",
       " 494: 'meat',\n",
       " 495: 'hiding',\n",
       " 496: 'alive',\n",
       " 497: 'white',\n",
       " 498: 'dance',\n",
       " 499: 'cheated',\n",
       " 500: 'confused',\n",
       " 501: 'black',\n",
       " 502: 'sweet',\n",
       " 503: 'cake',\n",
       " 504: 'japanese',\n",
       " 505: 'people',\n",
       " 506: 'raining',\n",
       " 507: 'minute',\n",
       " 508: 'keys',\n",
       " 509: 'breath',\n",
       " 510: 'idiot',\n",
       " 511: 'phone',\n",
       " 512: 'betrayed',\n",
       " 513: 'story',\n",
       " 514: 'hi',\n",
       " 515: 'agree',\n",
       " 516: 'cute',\n",
       " 517: 'save',\n",
       " 518: 'fly',\n",
       " 519: 'slowly',\n",
       " 520: 'tea',\n",
       " 521: 'single',\n",
       " 522: 'fight',\n",
       " 523: 'blame',\n",
       " 524: 'trapped',\n",
       " 525: 'party',\n",
       " 526: 'face',\n",
       " 527: 'rules',\n",
       " 528: 'music',\n",
       " 529: 'lonely',\n",
       " 530: 'last',\n",
       " 531: 'stopped',\n",
       " 532: 'explain',\n",
       " 533: 'asleep',\n",
       " 534: 'six',\n",
       " 535: 'guy',\n",
       " 536: 'brother',\n",
       " 537: 'any',\n",
       " 538: 'place',\n",
       " 539: 'started',\n",
       " 540: 'beat',\n",
       " 541: 'deep',\n",
       " 542: 'talked',\n",
       " 543: 'warn',\n",
       " 544: 'dying',\n",
       " 545: 'sign',\n",
       " 546: 'ice',\n",
       " 547: 'wrote',\n",
       " 548: 'count',\n",
       " 549: 'finish',\n",
       " 550: 'dumb',\n",
       " 551: 'often',\n",
       " 552: 'naked',\n",
       " 553: 'waiting',\n",
       " 554: 'anything',\n",
       " 555: 'became',\n",
       " 556: 'bath',\n",
       " 557: 'sang',\n",
       " 558: 'light',\n",
       " 559: 'listening',\n",
       " 560: 'touch',\n",
       " 561: 'head',\n",
       " 562: 'seem',\n",
       " 563: 'glasses',\n",
       " 564: 'sounds',\n",
       " 565: 'tie',\n",
       " 566: 'into',\n",
       " 567: 'couldn',\n",
       " 568: 'mouth',\n",
       " 569: 'being',\n",
       " 570: 'speaks',\n",
       " 571: 'slept',\n",
       " 572: 'welcome',\n",
       " 573: 'bald',\n",
       " 574: 'weak',\n",
       " 575: 'helps',\n",
       " 576: 'paid',\n",
       " 577: 'after',\n",
       " 578: 'inside',\n",
       " 579: 'cried',\n",
       " 580: 'forgive',\n",
       " 581: 'god',\n",
       " 582: 'survived',\n",
       " 583: 'moving',\n",
       " 584: 'sleepy',\n",
       " 585: 'green',\n",
       " 586: 'curious',\n",
       " 587: 'excited',\n",
       " 588: 'doll',\n",
       " 589: 'pain',\n",
       " 590: 'another',\n",
       " 591: 'arm',\n",
       " 592: 'losing',\n",
       " 593: 'fill',\n",
       " 594: 'gun',\n",
       " 595: 'comes',\n",
       " 596: 'tennis',\n",
       " 597: 'week',\n",
       " 598: 'advice',\n",
       " 599: 'important',\n",
       " 600: 'horse',\n",
       " 601: 'dry',\n",
       " 602: 'learn',\n",
       " 603: 'turned',\n",
       " 604: 'fishing',\n",
       " 605: 'canadian',\n",
       " 606: 'summer',\n",
       " 607: 'choice',\n",
       " 608: 'quite',\n",
       " 609: 'war',\n",
       " 610: 'cannot',\n",
       " 611: 'paper',\n",
       " 612: 'three',\n",
       " 613: 'smile',\n",
       " 614: 'fair',\n",
       " 615: 'catch',\n",
       " 616: 'weird',\n",
       " 617: 'ours',\n",
       " 618: 'careful',\n",
       " 619: 'serious',\n",
       " 620: 'hero',\n",
       " 621: 'liar',\n",
       " 622: 'normal',\n",
       " 623: 'trying',\n",
       " 624: 'cheese',\n",
       " 625: 'walked',\n",
       " 626: 'strong',\n",
       " 627: 'song',\n",
       " 628: 'shall',\n",
       " 629: 'sleeping',\n",
       " 630: 'horses',\n",
       " 631: 'student',\n",
       " 632: 'forgetful',\n",
       " 633: 'together',\n",
       " 634: 'second',\n",
       " 635: 'luck',\n",
       " 636: 'raise',\n",
       " 637: 'mistake',\n",
       " 638: 'clock',\n",
       " 639: 'trouble',\n",
       " 640: 'end',\n",
       " 641: 'getting',\n",
       " 642: 'noise',\n",
       " 643: 'maybe',\n",
       " 644: 'brown',\n",
       " 645: 'himself',\n",
       " 646: 'shirt',\n",
       " 647: 'okay',\n",
       " 648: 'poor',\n",
       " 649: 'doubt',\n",
       " 650: 'laugh',\n",
       " 651: 'strange',\n",
       " 652: 'news',\n",
       " 653: 'baffled',\n",
       " 654: 'cooking',\n",
       " 655: 'nervous',\n",
       " 656: 'worried',\n",
       " 657: 'mess',\n",
       " 658: 'invited',\n",
       " 659: 'quickly',\n",
       " 660: 'apples',\n",
       " 661: 'women',\n",
       " 662: 'thrilled',\n",
       " 663: 'writing',\n",
       " 664: 'closed',\n",
       " 665: 'boss',\n",
       " 666: 'thirsty',\n",
       " 667: 'deal',\n",
       " 668: 'satisfied',\n",
       " 669: 'worse',\n",
       " 670: 'duty',\n",
       " 671: 'simple',\n",
       " 672: 'cup',\n",
       " 673: 'also',\n",
       " 674: 'truck',\n",
       " 675: 'if',\n",
       " 676: 'throw',\n",
       " 677: 'killed',\n",
       " 678: 'apple',\n",
       " 679: 'letter',\n",
       " 680: 'sister',\n",
       " 681: 'teeth',\n",
       " 682: 'bicycle',\n",
       " 683: 'easily',\n",
       " 684: 'jump',\n",
       " 685: 'join',\n",
       " 686: 'wash',\n",
       " 687: 'waited',\n",
       " 688: 'ignore',\n",
       " 689: 'walks',\n",
       " 690: 'cruel',\n",
       " 691: 'bored',\n",
       " 692: 'town',\n",
       " 693: 'bread',\n",
       " 694: 'soup',\n",
       " 695: 'gas',\n",
       " 696: 'saved',\n",
       " 697: 'many',\n",
       " 698: 'kissed',\n",
       " 699: 'team',\n",
       " 700: 'bear',\n",
       " 701: 'trap',\n",
       " 702: 'snowing',\n",
       " 703: 'smoking',\n",
       " 704: 'pale',\n",
       " 705: 'dad',\n",
       " 706: 'nose',\n",
       " 707: 'kid',\n",
       " 708: 'trusted',\n",
       " 709: 'watched',\n",
       " 710: 'robbed',\n",
       " 711: 'favor',\n",
       " 712: 'feeling',\n",
       " 713: 'undressing',\n",
       " 714: 'used',\n",
       " 715: 'cheap',\n",
       " 716: 'wonderful',\n",
       " 717: 'heart',\n",
       " 718: 'blew',\n",
       " 719: 'swimming',\n",
       " 720: 'paris',\n",
       " 721: 'side',\n",
       " 722: 'lit',\n",
       " 723: 'embarrassed',\n",
       " 724: 'tonight',\n",
       " 725: 'voice',\n",
       " 726: 'table',\n",
       " 727: 'park',\n",
       " 728: 'parents',\n",
       " 729: 'drop',\n",
       " 730: 'ahead',\n",
       " 731: 'agreed',\n",
       " 732: 'birds',\n",
       " 733: 'step',\n",
       " 734: 'stood',\n",
       " 735: 'ill',\n",
       " 736: 'prepared',\n",
       " 737: 'closer',\n",
       " 738: 'hated',\n",
       " 739: 'share',\n",
       " 740: 'dancing',\n",
       " 741: 'healthy',\n",
       " 742: 'talks',\n",
       " 743: 'woke',\n",
       " 744: 'cheered',\n",
       " 745: 'evil',\n",
       " 746: 'half',\n",
       " 747: 'jealous',\n",
       " 748: 'divorced',\n",
       " 749: 'angel',\n",
       " 750: 'looking',\n",
       " 751: 'control',\n",
       " 752: 'dreaming',\n",
       " 753: 'caught',\n",
       " 754: 'rope',\n",
       " 755: 'accept',\n",
       " 756: 'mom',\n",
       " 757: 'budge',\n",
       " 758: 'danger',\n",
       " 759: 'copy',\n",
       " 760: 'japan',\n",
       " 761: 'law',\n",
       " 762: 'map',\n",
       " 763: 'dirty',\n",
       " 764: 'winning',\n",
       " 765: 'form',\n",
       " 766: 'rather',\n",
       " 767: 'baby',\n",
       " 768: 'excuse',\n",
       " 769: 'continue',\n",
       " 770: 'thirty',\n",
       " 771: 'wanted',\n",
       " 772: 'surprised',\n",
       " 773: 'bank',\n",
       " 774: 'brush',\n",
       " 775: 'legs',\n",
       " 776: 'interfering',\n",
       " 777: 'ridiculous',\n",
       " 778: 'relax',\n",
       " 779: 'slow',\n",
       " 780: 'goodbye',\n",
       " 781: 'wet',\n",
       " 782: 'grab',\n",
       " 783: 'spoke',\n",
       " 784: 'runs',\n",
       " 785: 'fantastic',\n",
       " 786: 'hers',\n",
       " 787: 'tight',\n",
       " 788: 'drove',\n",
       " 789: 'screamed',\n",
       " 790: 'scared',\n",
       " 791: 'cheat',\n",
       " 792: 'glad',\n",
       " 793: 'shoot',\n",
       " 794: 'grew',\n",
       " 795: 'cars',\n",
       " 796: 'nuts',\n",
       " 797: 'age',\n",
       " 798: 'or',\n",
       " 799: 'mind',\n",
       " 800: 'innocent',\n",
       " 801: 'restless',\n",
       " 802: 'closely',\n",
       " 803: 'leg',\n",
       " 804: 'point',\n",
       " 805: 'ended',\n",
       " 806: 'dreams',\n",
       " 807: 'bird',\n",
       " 808: 'type',\n",
       " 809: 'american',\n",
       " 810: 'guilty',\n",
       " 811: 'finally',\n",
       " 812: 'soccer',\n",
       " 813: 'spring',\n",
       " 814: 'behind',\n",
       " 815: 'exhausted',\n",
       " 816: 'babbling',\n",
       " 817: 'ball',\n",
       " 818: 'knife',\n",
       " 819: 'genius',\n",
       " 820: 'glass',\n",
       " 821: 'laughing',\n",
       " 822: 'sugar',\n",
       " 823: 'correct',\n",
       " 824: 'says',\n",
       " 825: 'seemed',\n",
       " 826: 'brothers',\n",
       " 827: 'husband',\n",
       " 828: 'breathe',\n",
       " 829: 'writes',\n",
       " 830: 'matter',\n",
       " 831: 'cousin',\n",
       " 832: 'proud',\n",
       " 833: 'yesterday',\n",
       " 834: 'taught',\n",
       " 835: 'train',\n",
       " 836: 'oh',\n",
       " 837: 'bowed',\n",
       " 838: 'phoned',\n",
       " 839: 'refuse',\n",
       " 840: 'stayed',\n",
       " 841: 'lazy',\n",
       " 842: 'lies',\n",
       " 843: 'fainted',\n",
       " 844: 'then',\n",
       " 845: 'carry',\n",
       " 846: 'envy',\n",
       " 847: 'eaten',\n",
       " 848: 'sharp',\n",
       " 849: 'jumped',\n",
       " 850: 'even',\n",
       " 851: 'worry',\n",
       " 852: 'fruit',\n",
       " 853: 'rice',\n",
       " 854: 'golf',\n",
       " 855: 'unlucky',\n",
       " 856: 'escaped',\n",
       " 857: 'refused',\n",
       " 858: 'annoying',\n",
       " 859: 'doubts',\n",
       " 860: 'misled',\n",
       " 861: 'sue',\n",
       " 862: 'ears',\n",
       " 863: 'pregnant',\n",
       " 864: 'worn',\n",
       " 865: 'amazing',\n",
       " 866: 'smiling',\n",
       " 867: 'feet',\n",
       " 868: 'command',\n",
       " 869: 'built',\n",
       " 870: 'sent',\n",
       " 871: 'deny',\n",
       " 872: 'sells',\n",
       " 873: 'fear',\n",
       " 874: 'nearly',\n",
       " 875: 'rescued',\n",
       " 876: 'allow',\n",
       " 877: 'diet',\n",
       " 878: 'famous',\n",
       " 879: 'gets',\n",
       " 880: 'hoax',\n",
       " 881: 'unfair',\n",
       " 882: 'lock',\n",
       " 883: 'number',\n",
       " 884: 'stingy',\n",
       " 885: 'helping',\n",
       " 886: 'special',\n",
       " 887: 'kite',\n",
       " 888: 'sons',\n",
       " 889: 'sun',\n",
       " 890: 'teach',\n",
       " 891: 'heavy',\n",
       " 892: 'desk',\n",
       " 893: 'cooks',\n",
       " 894: 'bike',\n",
       " 895: 'ticket',\n",
       " 896: 'creep',\n",
       " 897: 'bite',\n",
       " 898: 'carefully',\n",
       " 899: 'honest',\n",
       " 900: 'death',\n",
       " 901: 'fever',\n",
       " 902: 'policeman',\n",
       " 903: 'defenseless',\n",
       " 904: 'dangerous',\n",
       " 905: 'radio',\n",
       " 906: 'sky',\n",
       " 907: 'cap',\n",
       " 908: 'singer',\n",
       " 909: 'hotel',\n",
       " 910: 'drinking',\n",
       " 911: 'intelligent',\n",
       " 912: 'pick',\n",
       " 913: 'housesitting',\n",
       " 914: 'children',\n",
       " 915: 'thing',\n",
       " 916: 'weren',\n",
       " 917: 'stuck',\n",
       " 918: 'fit',\n",
       " 919: 'brief',\n",
       " 920: 'awful',\n",
       " 921: 'burned',\n",
       " 922: 'seize',\n",
       " 923: 'voted',\n",
       " 924: 'burns',\n",
       " 925: 'human',\n",
       " 926: 'skinny',\n",
       " 927: 'happens',\n",
       " 928: 'magic',\n",
       " 929: 'windy',\n",
       " 930: 'vote',\n",
       " 931: 'deaf',\n",
       " 932: 'ugly',\n",
       " 933: 'wood',\n",
       " 934: 'decide',\n",
       " 935: 'lips',\n",
       " 936: 'loser',\n",
       " 937: 'rule',\n",
       " 938: 'thief',\n",
       " 939: 'huge',\n",
       " 940: 'crashed',\n",
       " 941: 'upset',\n",
       " 942: 'twins',\n",
       " 943: 'pity',\n",
       " 944: 'confident',\n",
       " 945: 'upstairs',\n",
       " 946: 'exist',\n",
       " 947: 'framed',\n",
       " 948: 'sushi',\n",
       " 949: 'games',\n",
       " 950: 'cab',\n",
       " 951: 'adult',\n",
       " 952: 'shopping',\n",
       " 953: 'burn',\n",
       " 954: 'shame',\n",
       " 955: 'focused',\n",
       " 956: 'boys',\n",
       " 957: 'answered',\n",
       " 958: 'canceled',\n",
       " 959: 'waste',\n",
       " 960: 'years',\n",
       " 961: 'rude',\n",
       " 962: 'tough',\n",
       " 963: 'body',\n",
       " 964: 'rush',\n",
       " 965: 'few',\n",
       " 966: 'smells',\n",
       " 967: 'cancer',\n",
       " 968: 'sports',\n",
       " 969: 'supper',\n",
       " 970: 'ride',\n",
       " 971: 'easygoing',\n",
       " 972: 'charge',\n",
       " 973: 'takes',\n",
       " 974: 'theirs',\n",
       " 975: 'adores',\n",
       " 976: 'remembers',\n",
       " 977: 'moment',\n",
       " 978: 'thinks',\n",
       " 979: 'sound',\n",
       " 980: 'cows',\n",
       " 981: 'grass',\n",
       " 982: 'towel',\n",
       " 983: 'year',\n",
       " 984: 'uncle',\n",
       " 985: 'animal',\n",
       " 986: 'disgusting',\n",
       " 987: 'borrow',\n",
       " 988: 'arrested',\n",
       " 989: 'sisters',\n",
       " 990: 'chicken',\n",
       " 991: 'travel',\n",
       " 992: 'tokyo',\n",
       " 993: 'having',\n",
       " 994: 'wide',\n",
       " 995: 'likely',\n",
       " 996: 'delicious',\n",
       " 997: 'their',\n",
       " 998: 'bill',\n",
       " 999: 'ruined',\n",
       " 1000: 'missing',\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_lang_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bd31d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(input_lang_tokenizer.word_index) + 1\n",
    "vocab_targ_size = len(target_lang_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7ddc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dc5bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True)\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a4d1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c1e2536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Encoder at 0x17c872fd730>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd3a3ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_hidden = encoder.initialize_hidden_state()\n",
    "simple_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a52db999",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_output_batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1830cf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 16, 1024), dtype=float32, numpy=\n",
       " array([[[-3.81078478e-03, -6.88044401e-03, -4.75821132e-03, ...,\n",
       "           2.65271030e-03,  6.67464873e-03, -2.40614102e-03],\n",
       "         [-1.60770165e-03, -4.46899794e-04, -6.26616040e-03, ...,\n",
       "          -1.01432868e-03, -6.45269174e-05, -3.36275832e-03],\n",
       "         [-6.00970117e-03, -9.80717875e-03, -7.96431582e-03, ...,\n",
       "           2.46365787e-03, -1.56786220e-04,  4.06825356e-03],\n",
       "         ...,\n",
       "         [-1.41691770e-02, -7.67568592e-03,  1.21137165e-02, ...,\n",
       "           7.87413213e-04,  1.88760902e-03,  6.56944234e-03],\n",
       "         [-1.43058458e-02, -7.98291527e-03,  1.25614312e-02, ...,\n",
       "           4.72557207e-04,  2.13799253e-03,  6.56569190e-03],\n",
       "         [-1.43625867e-02, -8.19821469e-03,  1.28151607e-02, ...,\n",
       "           2.35170693e-04,  2.27858033e-03,  6.58854609e-03]],\n",
       " \n",
       "        [[-3.81078478e-03, -6.88044401e-03, -4.75821132e-03, ...,\n",
       "           2.65271030e-03,  6.67464873e-03, -2.40614102e-03],\n",
       "         [-1.15837893e-02, -1.20600238e-02, -1.87902749e-02, ...,\n",
       "           3.27207916e-03, -2.58949120e-03,  8.84880684e-03],\n",
       "         [-4.23982553e-03, -3.12787946e-03, -1.56184379e-02, ...,\n",
       "          -4.04525734e-03, -1.06438678e-02, -4.65315301e-04],\n",
       "         ...,\n",
       "         [-1.42362146e-02, -8.25615413e-03,  1.27773071e-02, ...,\n",
       "           3.60135949e-04,  2.18832237e-03,  6.54614531e-03],\n",
       "         [-1.43033331e-02, -8.39044899e-03,  1.29370838e-02, ...,\n",
       "           1.42036151e-04,  2.29364866e-03,  6.59543555e-03],\n",
       "         [-1.43415425e-02, -8.48237984e-03,  1.30261471e-02, ...,\n",
       "          -8.88937939e-06,  2.34723184e-03,  6.63851667e-03]],\n",
       " \n",
       "        [[-3.81078478e-03, -6.88044401e-03, -4.75821132e-03, ...,\n",
       "           2.65271030e-03,  6.67464873e-03, -2.40614102e-03],\n",
       "         [ 9.28477570e-03, -1.18429214e-03, -1.69329566e-03, ...,\n",
       "           6.06555631e-03,  9.79122333e-03,  6.78761490e-03],\n",
       "         [ 1.07184593e-02,  1.03063602e-02,  1.25577543e-02, ...,\n",
       "           3.73934349e-03,  2.81113479e-03,  1.48379314e-03],\n",
       "         ...,\n",
       "         [-1.43091129e-02, -8.14847648e-03,  1.26021337e-02, ...,\n",
       "           5.07781806e-04,  2.04274850e-03,  6.62816921e-03],\n",
       "         [-1.43652363e-02, -8.30581691e-03,  1.28393676e-02, ...,\n",
       "           2.61631096e-04,  2.20922055e-03,  6.63260184e-03],\n",
       "         [-1.43869631e-02, -8.41906853e-03,  1.29719572e-02, ...,\n",
       "           8.20005080e-05,  2.30167178e-03,  6.65213726e-03]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-3.81078478e-03, -6.88044401e-03, -4.75821132e-03, ...,\n",
       "           2.65271030e-03,  6.67464873e-03, -2.40614102e-03],\n",
       "         [-1.70024112e-03, -8.48215725e-03, -1.48206018e-05, ...,\n",
       "          -5.72828483e-03,  3.19185248e-03,  4.21179226e-04],\n",
       "         [ 5.32479957e-03,  7.42111262e-03, -3.45279579e-03, ...,\n",
       "          -1.24745555e-02,  6.28352340e-04,  5.97152114e-03],\n",
       "         ...,\n",
       "         [-1.42260445e-02, -7.89922848e-03,  1.25523545e-02, ...,\n",
       "           5.33524668e-04,  1.92030857e-03,  6.62716106e-03],\n",
       "         [-1.43036693e-02, -8.14776681e-03,  1.28092766e-02, ...,\n",
       "           2.64035014e-04,  2.13636411e-03,  6.63617440e-03],\n",
       "         [-1.43454112e-02, -8.32028314e-03,  1.29543375e-02, ...,\n",
       "           7.54387438e-05,  2.25654640e-03,  6.65540434e-03]],\n",
       " \n",
       "        [[-3.81078478e-03, -6.88044401e-03, -4.75821132e-03, ...,\n",
       "           2.65271030e-03,  6.67464873e-03, -2.40614102e-03],\n",
       "         [-2.31051887e-03,  1.64545770e-03, -1.26331700e-02, ...,\n",
       "          -1.51572353e-03, -2.82918732e-03, -5.11284277e-04],\n",
       "         [-2.35051336e-03, -9.50226677e-04, -1.35457003e-02, ...,\n",
       "           7.83464499e-03,  3.02404515e-04,  3.84797994e-03],\n",
       "         ...,\n",
       "         [-1.43991755e-02, -8.41998588e-03,  1.29188718e-02, ...,\n",
       "           1.02861071e-04,  2.30326690e-03,  6.65590400e-03],\n",
       "         [-1.44042000e-02, -8.49778205e-03,  1.30118392e-02, ...,\n",
       "          -3.16010592e-05,  2.35361652e-03,  6.67501707e-03],\n",
       "         [-1.44049516e-02, -8.55197757e-03,  1.30648548e-02, ...,\n",
       "          -1.23890699e-04,  2.37585464e-03,  6.69435272e-03]],\n",
       " \n",
       "        [[-3.81078478e-03, -6.88044401e-03, -4.75821132e-03, ...,\n",
       "           2.65271030e-03,  6.67464873e-03, -2.40614102e-03],\n",
       "         [ 3.15356115e-03, -5.47020417e-03, -8.19250196e-03, ...,\n",
       "          -1.53463101e-04,  3.39655904e-04, -1.77975441e-03],\n",
       "         [-2.10505538e-03, -1.26974154e-02, -9.08946246e-03, ...,\n",
       "           4.21056664e-03, -3.82208906e-04,  5.48048085e-03],\n",
       "         ...,\n",
       "         [-1.42647587e-02, -8.30815360e-03,  1.27666276e-02, ...,\n",
       "           3.20024759e-04,  2.22570798e-03,  6.59508165e-03],\n",
       "         [-1.43250013e-02, -8.41687061e-03,  1.29248165e-02, ...,\n",
       "           1.11048481e-04,  2.31703534e-03,  6.62208535e-03],\n",
       "         [-1.43577233e-02, -8.49526934e-03,  1.30147934e-02, ...,\n",
       "          -3.13484015e-05,  2.36124173e-03,  6.65210187e-03]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       " array([[-1.43625867e-02, -8.19821469e-03,  1.28151607e-02, ...,\n",
       "          2.35170693e-04,  2.27858033e-03,  6.58854609e-03],\n",
       "        [-1.43415425e-02, -8.48237984e-03,  1.30261471e-02, ...,\n",
       "         -8.88937939e-06,  2.34723184e-03,  6.63851667e-03],\n",
       "        [-1.43869631e-02, -8.41906853e-03,  1.29719572e-02, ...,\n",
       "          8.20005080e-05,  2.30167178e-03,  6.65213726e-03],\n",
       "        ...,\n",
       "        [-1.43454112e-02, -8.32028314e-03,  1.29543375e-02, ...,\n",
       "          7.54387438e-05,  2.25654640e-03,  6.65540434e-03],\n",
       "        [-1.44049516e-02, -8.55197757e-03,  1.30648548e-02, ...,\n",
       "         -1.23890699e-04,  2.37585464e-03,  6.69435272e-03],\n",
       "        [-1.43577233e-02, -8.49526934e-03,  1.30147934e-02, ...,\n",
       "         -3.13484015e-05,  2.36124173e-03,  6.65210187e-03]], dtype=float32)>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(example_input_batch, simple_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5829ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_output, simple_states = encoder(example_input_batch, simple_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61ed7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(keras.layers.Layer):\n",
    "    def __init__ (self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "    def call (self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        atteion_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = atteion_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, atteion_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9531f05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       " array([[-0.00637142, -0.0011614 ,  0.00246351, ...,  0.00320543,\n",
       "          0.00173788,  0.0025068 ],\n",
       "        [-0.00713754, -0.00466628,  0.00087075, ...,  0.00310231,\n",
       "         -0.00075256,  0.00410261],\n",
       "        [-0.00434387, -0.00376534,  0.00662781, ...,  0.00301996,\n",
       "          0.00232902,  0.00440815],\n",
       "        ...,\n",
       "        [-0.00694919, -0.00345998,  0.00228049, ..., -0.00040365,\n",
       "         -0.00039935,  0.00284764],\n",
       "        [-0.0085916 , -0.00443213,  0.00328777, ...,  0.00262704,\n",
       "          0.00125907,  0.00528511],\n",
       "        [-0.00681534, -0.0058036 ,  0.00294724, ...,  0.00479816,\n",
       "          0.00115968,  0.00433635]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 16, 1), dtype=float32, numpy=\n",
       " array([[[0.06260864],\n",
       "         [0.06200516],\n",
       "         [0.06119423],\n",
       "         ...,\n",
       "         [0.06337715],\n",
       "         [0.0633753 ],\n",
       "         [0.06337006]],\n",
       " \n",
       "        [[0.06236051],\n",
       "         [0.06222363],\n",
       "         [0.06182159],\n",
       "         ...,\n",
       "         [0.06311776],\n",
       "         [0.06311247],\n",
       "         [0.06310842]],\n",
       " \n",
       "        [[0.06260805],\n",
       "         [0.06197914],\n",
       "         [0.06158543],\n",
       "         ...,\n",
       "         [0.0633672 ],\n",
       "         [0.06336342],\n",
       "         [0.0633596 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.0623197 ],\n",
       "         [0.06323407],\n",
       "         [0.06244686],\n",
       "         ...,\n",
       "         [0.06307428],\n",
       "         [0.06307167],\n",
       "         [0.06306827]],\n",
       " \n",
       "        [[0.06240765],\n",
       "         [0.06146565],\n",
       "         [0.0617351 ],\n",
       "         ...,\n",
       "         [0.06315601],\n",
       "         [0.06315363],\n",
       "         [0.06315178]],\n",
       " \n",
       "        [[0.06236988],\n",
       "         [0.06258639],\n",
       "         [0.06147289],\n",
       "         ...,\n",
       "         [0.06312675],\n",
       "         [0.06312219],\n",
       "         [0.06311839]]], dtype=float32)>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer = Attention(10)\n",
    "attention_layer(simple_hidden, simple_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0ef2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_result, attention_weights = attention_layer(simple_hidden, simple_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d09637b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__ (self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.attention = Attention(self.dec_units)\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "630af304",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_targ_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17f088aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Decoder at 0x17c8afc8550>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de54dba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 3727), dtype=float32, numpy=\n",
       " array([[-5.9663528e-04, -9.2281611e-04,  3.1096116e-03, ...,\n",
       "         -4.0878035e-04,  1.3587471e-03,  1.0131508e-03],\n",
       "        [-1.0738734e-03, -1.0389721e-03,  2.9886661e-03, ...,\n",
       "         -3.3308711e-04,  1.5985999e-03,  6.8748649e-04],\n",
       "        [-1.0771633e-03, -8.5626956e-04,  3.0706492e-03, ...,\n",
       "         -5.6035235e-05,  1.7186156e-03,  1.8436906e-03],\n",
       "        ...,\n",
       "        [-1.0118932e-03, -1.1757703e-03,  2.6470064e-03, ...,\n",
       "          2.2588432e-04,  1.1720227e-03,  1.2730743e-03],\n",
       "        [-1.6822740e-03, -1.3195685e-03,  2.7264822e-03, ...,\n",
       "         -2.7071417e-04,  1.8500630e-03,  1.7058863e-03],\n",
       "        [-1.0642146e-03, -7.3215301e-04,  3.0915393e-03, ...,\n",
       "         -2.2158367e-04,  2.1131495e-03,  8.6248538e-04]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       " array([[ 0.00581166,  0.00582146,  0.00228046, ..., -0.00480631,\n",
       "         -0.00498638,  0.00283025],\n",
       "        [ 0.00568624,  0.00521999,  0.00167455, ..., -0.00543296,\n",
       "         -0.00626067,  0.00227441],\n",
       "        [ 0.00518382,  0.00475371,  0.00205795, ..., -0.00500844,\n",
       "         -0.00495481,  0.00225816],\n",
       "        ...,\n",
       "        [ 0.00546706,  0.00485778,  0.00210435, ..., -0.00521357,\n",
       "         -0.00482509,  0.00288096],\n",
       "        [ 0.00510447,  0.00509394,  0.0012757 , ..., -0.00557965,\n",
       "         -0.0051176 ,  0.00145796],\n",
       "        [ 0.00461476,  0.0057934 ,  0.00203114, ..., -0.00479812,\n",
       "         -0.00586204,  0.00170804]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 16, 1), dtype=float32, numpy=\n",
       " array([[[0.06272441],\n",
       "         [0.0629467 ],\n",
       "         [0.06271146],\n",
       "         ...,\n",
       "         [0.06272211],\n",
       "         [0.06272283],\n",
       "         [0.06272154]],\n",
       " \n",
       "        [[0.06261549],\n",
       "         [0.06259315],\n",
       "         [0.06218823],\n",
       "         ...,\n",
       "         [0.06260879],\n",
       "         [0.0626085 ],\n",
       "         [0.06260783]],\n",
       " \n",
       "        [[0.06267013],\n",
       "         [0.06362813],\n",
       "         [0.06317923],\n",
       "         ...,\n",
       "         [0.06266294],\n",
       "         [0.06266475],\n",
       "         [0.06266458]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.06254917],\n",
       "         [0.06266687],\n",
       "         [0.06226084],\n",
       "         ...,\n",
       "         [0.06254496],\n",
       "         [0.06254438],\n",
       "         [0.06254314]],\n",
       " \n",
       "        [[0.06271633],\n",
       "         [0.06226714],\n",
       "         [0.06172331],\n",
       "         ...,\n",
       "         [0.06270985],\n",
       "         [0.06270878],\n",
       "         [0.06270787]],\n",
       " \n",
       "        [[0.06252651],\n",
       "         [0.06271937],\n",
       "         [0.06262714],\n",
       "         ...,\n",
       "         [0.06252332],\n",
       "         [0.06252143],\n",
       "         [0.06251977]]], dtype=float32)>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(tf.random.uniform((BATCH_SIZE, 1)), simple_hidden, simple_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7203cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cd635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94d9c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'chckpnts'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1b69f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa8a00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH = 20\n",
    "# for epoch in range(EPOCH):\n",
    "#     enc_hidden = encoder.initialize_hidden_state()\n",
    "#     total_loss = 0\n",
    "#     for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "#         batch_loss = train_step(inp, targ, enc_hidden)\n",
    "#         total_loss += batch_loss\n",
    "#         print('Epoch: ', epoch)\n",
    "#         print('Loss: ', batch_loss.numpy())\n",
    "#     checkpoint.save(file_prefix='chckpnts/test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc85622c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x17c8b1b3a30>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint('chckpnts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55517bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequence([inputs], maxlen=max_len_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)\n",
    "    for t in range(max_len_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += target_lang_tokenizer[predicted_id] + ' '\n",
    "        if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5f240d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('it s very cold here . <end> ', '<start> hace mucho frio aqui . <end>')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('hace mucho frio aqui.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
